{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496f3024",
   "metadata": {},
   "source": [
    "# You're Toxic, I'm Slippin' Under: Toxic Comment Classification Challenge\n",
    "\n",
    "#### STINTSY S13 Group 8\n",
    "- VICENTE, Francheska Josefa\n",
    "- VISTA, Sophia Danielle S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c582a",
   "metadata": {},
   "source": [
    "## Requirements and Imports\n",
    "Before starting, the relevant libraries and files in building and training the model should be loaded into the notebook first.\n",
    "\n",
    "### Import\n",
    "Several libraries are required to perform a thorough analysis of the dataset. Each of these libraries will be imported and described below:\n",
    "\n",
    "#### Basic Libraries \n",
    "Import `numpy` and `pandas`.\n",
    "- `numpy` contains a large collection of mathematical functions\n",
    "- `pandas` contains functions that are designed for data manipulation and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7797c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6255319",
   "metadata": {},
   "source": [
    "#### Natural Language Processing Libraries \n",
    "- `re` is a module that allows the use of regular expressions\n",
    "- `nltk` provides functions for processing text data\n",
    "- `stopwords` is a corpus from NLTK, which includes a compiled list of stopwords\n",
    "- `Counter` is from Python's `collections` module, which is helpful for tokenization\n",
    "- `string` contains functions for string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce303532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c181e3",
   "metadata": {},
   "source": [
    "#### Machine Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836013db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-multilearn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-multilearn\n",
    "\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe4ba0e",
   "metadata": {},
   "source": [
    "### Datasets and Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8424525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('cleaned_data/cleaned_train.csv')\n",
    "test = pd.read_csv('cleaned_data/cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15478728",
   "metadata": {},
   "source": [
    "## Trying different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fafb6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61100a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, actual):\n",
    "    accuracy = np.sum (predictions == actual) / len (predictions) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567593f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967d71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train.columns [2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160c31b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03b0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test ['comment_text'] = test ['comment_text'].apply(lambda x: np.str_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84367b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c232fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train ['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98318366",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176302c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_hyper_parameter:\n",
    "    def __init__(self, class_, alpha, fit_prior):\n",
    "        self.class_ = class_\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c247db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_hyperparameter:\n",
    "    def __init__(self, class_, c, max_iter):\n",
    "        self.class_ = class_\n",
    "        self.c = c\n",
    "        self.max_iter = max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df21c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cdca5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613bf342",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f9a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0ad45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c9749e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words = 'english', max_features = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0caa548",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ebad703",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b3b66",
   "metadata": {},
   "source": [
    "### Classifier Chain: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07effd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.loc [ : , 'toxic' : ]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cc = ClassifierChain(\n",
    "    classifier = LogisticRegression(max_iter = 300, C = 10),\n",
    ")\n",
    "\n",
    "lr_cc.fit(count_train, y_train)\n",
    "\n",
    "predictions = lr_cc.predict(count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions.todense(), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "\n",
    "predictions = lr_cc.predict(count_test)\n",
    "predictions = predictions.todense()\n",
    "for i in range (6):\n",
    "    sample_submission[classes [i]] = predictions[:, i : i + 1]\n",
    "\n",
    "sample_submission.to_csv(f'results/submission_class_lr_cc.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bc68f",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d79b2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_lr = [\n",
    "    {\n",
    "        'classifier': [LogisticRegression()],\n",
    "        'classifier__C': [1, 12, 15],\n",
    "        'classifier__max_iter': [600, 1800, 3000]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67034465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "lr_cc_tuned = RandomizedSearchCV(ClassifierChain(), parameters_lr, scoring = 'accuracy', n_jobs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7bef7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.9 GiB for an array with shape (159571, 10000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4e12a19e8f16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlr_cc_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlr_cc_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_cc_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skmultilearn\\problem_transform\\cc.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, order)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0my_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_data_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             self.classifiers_[label] = self.classifier.fit(self._ensure_input_format(\n\u001b[0m\u001b[0;32m    155\u001b[0m                 X_extended), self._ensure_output_format(y_subset))\n\u001b[0;32m    156\u001b[0m             \u001b[0mX_extended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_extended\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_subset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[0;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.9 GiB for an array with shape (159571, 10000) and data type float64"
     ]
    }
   ],
   "source": [
    "# train\n",
    "lr_cc_tuned.fit(count_train, y_train)\n",
    "print (lr_cc_tuned.best_params_, lr_cc_tuned.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_cc_tuned.predict(count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions.todense(), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "predictions = lr_cc_tuned.predict(count_test)\n",
    "predictions = predictions.todense()\n",
    "for i in range (6):\n",
    "    sample_submission[classes [i]] = predictions[:, i : i + 1]\n",
    "\n",
    "sample_submission.to_csv(f'results/submission_class_lr_cc_tuned.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cf14d",
   "metadata": {},
   "source": [
    "### Classifier Chain: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5054ee",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.loc [ : , 'toxic' : ]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_cc = ClassifierChain(\n",
    "    classifier = MultinomialNB(),\n",
    "    max_iter = 300,\n",
    "    alpha = 1.0,\n",
    "    fit_prior = True\n",
    ")\n",
    "\n",
    "mn_cc.fit(count_train, y_train)\n",
    "\n",
    "predictions = mn_cc.predict(count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions.todense(), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "predictions = mn_cc.predict(count_test)\n",
    "predictions = predictions.todense()\n",
    "for i in range (6):\n",
    "    sample_submission[classes [i]] = predictions[:, i : i + 1]\n",
    "\n",
    "sample_submission.to_csv(f'results/submission_class_mn_cc.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c2f29",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_mn = [\n",
    "    {\n",
    "        'classifier': [MultinomialNB()],\n",
    "        'classifier__alpha': [0.7, 1.0],\n",
    "        'classifier__fit_prior': [True, False]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_cc_tuned = RandomizedSearchCV(ClassifierChain(), parameters_mn, scoring = 'accuracy', n_jobs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "mn_cc_tuned.fit(count_train, y_train)\n",
    "print (mn_cc_tuned.best_params_, mn_cc_tuned.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a93384",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mn_cc_tuned.predict(count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions.todense(), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ab6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "predictions = mn_cc_tuned.predict(count_test)\n",
    "predictions = predictions.todense()\n",
    "for i in range (6):\n",
    "    sample_submission[classes [i]] = predictions[:, i : i + 1]\n",
    "\n",
    "sample_submission.to_csv(f'results/submission_class_mn_cc_tuned.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d378d29",
   "metadata": {},
   "source": [
    "### Binary Relevance: Logistic Regression using Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a08388",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "938c8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1600c74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           0             0        0       0       0              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "159566      0             0        0       0       0              0\n",
       "159567      0             0        0       0       0              0\n",
       "159568      0             0        0       0       0              0\n",
       "159569      0             0        0       0       0              0\n",
       "159570      0             0        0       0       0              0\n",
       "\n",
       "[159571 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train.loc [ : , 'toxic' : ]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "267bbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42d575ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_lr = BinaryRelevance(classifier = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_lr.fit(count_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d81a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = binary_lr.predict(count_train)\n",
    "print(compute_accuracy(predictions.todense(), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eab1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "predictions = binary_lr.predict(count_test)\n",
    "predictions = predictions.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a56dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (6):\n",
    "    sample_submission[classes [i]] = predictions[:, i : i + 1]\n",
    "\n",
    "sample_submission.to_csv(f'results/submission_binary_lr_count.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ecf99f",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11500cc5",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443a9a1",
   "metadata": {},
   "source": [
    "### Binary Relevance: Multinomial Naive Bayes using Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0662b",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24517619",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3c2ed7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           0             0        0       0       0              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "159566      0             0        0       0       0              0\n",
       "159567      0             0        0       0       0              0\n",
       "159568      0             0        0       0       0              0\n",
       "159569      0             0        0       0       0              0\n",
       "159570      0             0        0       0       0              0\n",
       "\n",
       "[159571 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train.loc [ : , 'toxic' : ]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f1b505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e83f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mn = BinaryRelevance(classifier = MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93a8e065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=MultinomialNB(), require_dense=[True, True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_mn.fit(count_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a4c7cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic            95.040452\n",
      "severe_toxic     98.326137\n",
      "obscene          97.008228\n",
      "threat           98.979764\n",
      "insult           96.499364\n",
      "identity_hate    98.079852\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "predictions = binary_mn.predict(count_train)\n",
    "print(compute_accuracy(predictions.todense(), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af6c030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "predictions = binary_mn.predict(count_test)\n",
    "predictions = predictions.todense()\n",
    "for i in range (6):\n",
    "    sample_submission[classes [i]] = predictions[:, i : i + 1]\n",
    "\n",
    "sample_submission.to_csv('results/submission_binary_mn_count.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c4564",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24293dcf",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde37f9",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf324509",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82f66a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e7b6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3612f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50fc9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    y_train = train[class_]\n",
    "    model = MultinomialNB ()\n",
    "    model.fit(tf_idf_train, y_train)\n",
    "    predictions = model.predict(tf_idf_train)\n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bc574c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "95.135080935759\n",
      "Class:  severe_toxic\n",
      "99.07940665910473\n",
      "Class:  obscene\n",
      "97.44753119301126\n",
      "Class:  threat\n",
      "99.70295354419036\n",
      "Class:  insult\n",
      "96.97689429783607\n",
      "Class:  identity_hate\n",
      "99.18594230781282\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    predictions = arr_model [counter].predict(tf_idf_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8aef6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(tf_idf_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "    \n",
    "sample_submission.to_csv(f'results/submission_tfidf_nb.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a8714",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac18a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac3e92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [{\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000], \n",
    "    'fit_prior' : [False, True]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8acd741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "Best accuracy:  95.00162935853407 %\n",
      "Best grid:  {'alpha': 0.1, 'fit_prior': True}\n",
      "Class:  severe_toxic\n",
      "Best accuracy:  99.0675055774196 %\n",
      "Best grid:  {'alpha': 1, 'fit_prior': True}\n",
      "Class:  obscene\n",
      "Best accuracy:  97.27270448449603 %\n",
      "Best grid:  {'alpha': 0.1, 'fit_prior': True}\n",
      "Class:  threat\n",
      "Best accuracy:  99.69919534755472 %\n",
      "Best grid:  {'alpha': 1, 'fit_prior': True}\n",
      "Class:  insult\n",
      "Best accuracy:  96.82400421126513 %\n",
      "Best grid:  {'alpha': 0.1, 'fit_prior': True}\n",
      "Class:  identity_hate\n",
      "Best accuracy:  99.18030732208658 %\n",
      "Best grid:  {'alpha': 1, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "final_hyperparameters = []\n",
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    best_score = 0\n",
    "    \n",
    "    model = MultinomialNB ()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split (X, y_train, random_state = 42, test_size = 0.25, stratify = y_train)\n",
    "    \n",
    "    X_train_sparse_matrix = tf_idf_vectorizer.fit_transform(X_train)\n",
    "    X_validation_sparse_matrix = tf_idf_vectorizer.transform(X_val)\n",
    "    \n",
    "    for g in ParameterGrid(hyperparameters):\n",
    "\n",
    "        model.set_params(**g)\n",
    "\n",
    "        model.fit(X_train_sparse_matrix, y_train)\n",
    "        predictions = model.predict (X_train_sparse_matrix)\n",
    "        train_acc = compute_accuracy (predictions, y_train)\n",
    "\n",
    "        predictions = model.predict (X_validation_sparse_matrix)\n",
    "        val_acc = compute_accuracy (predictions, y_val)\n",
    "\n",
    "        if val_acc > best_score:\n",
    "            best_score = val_acc\n",
    "            best_grid = g\n",
    "    \n",
    "    print(\"Best accuracy: \", best_score, \"%\")\n",
    "    print(\"Best grid: \", best_grid)\n",
    "    temp = mn_hyper_parameter (class_, best_grid['alpha'], best_grid['fit_prior'])\n",
    "    final_hyperparameters.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5cdd89",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a6bcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f41b3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf_vectorizer.fit_transform(X_train)\n",
    "tf_idf_test = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53d69379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "95.2246962167311\n",
      "Class:  severe_toxic\n",
      "99.07940665910473\n",
      "Class:  obscene\n",
      "97.48763873134843\n",
      "Class:  threat\n",
      "99.70295354419036\n",
      "Class:  insult\n",
      "97.01073503330807\n",
      "Class:  identity_hate\n",
      "99.18594230781282\n"
     ]
    }
   ],
   "source": [
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    \n",
    "    y_train = train[class_]\n",
    "    temp = final_hyperparameters [counter]\n",
    "    model = MultinomialNB (alpha = temp.alpha, fit_prior = temp.fit_prior)\n",
    "\n",
    "    model.fit(tf_idf_train, y_train)\n",
    "    predictions = model.predict(tf_idf_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    \n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2508e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(tf_idf_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "    \n",
    "sample_submission.to_csv(f'results/submission_tf_idf_mn_tuned.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab42516",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes using Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24309156",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "932e5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "490395e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "235b9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d67ea51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    y_train = train[class_]\n",
    "    model = MultinomialNB ()\n",
    "    model.fit(count_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(count_train)\n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d35f5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "94.87250189570786\n",
      "Class:  severe_toxic\n",
      "98.35057748588403\n",
      "Class:  obscene\n",
      "97.05146925193175\n",
      "Class:  threat\n",
      "98.91396306346391\n",
      "Class:  insult\n",
      "96.49059039549793\n",
      "Class:  identity_hate\n",
      "98.14627971247909\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    predictions = arr_model [counter].predict(count_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2d72d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(count_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "sample_submission.to_csv(f'results/submission_count_nb.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6d970",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42557419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9035d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [{\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000],\n",
    "    'fit_prior' : [False, True]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a34f8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "Best accuracy:  94.85624044318553 %\n",
      "Best grid:  {'alpha': 10, 'fit_prior': True}\n",
      "Class:  severe_toxic\n",
      "Best accuracy:  99.01486476324168 %\n",
      "Best grid:  {'alpha': 1000, 'fit_prior': True}\n",
      "Class:  obscene\n",
      "Best accuracy:  97.12480886371043 %\n",
      "Best grid:  {'alpha': 10, 'fit_prior': True}\n",
      "Class:  threat\n",
      "Best accuracy:  99.6641014714361 %\n",
      "Best grid:  {'alpha': 1000, 'fit_prior': True}\n",
      "Class:  insult\n",
      "Best accuracy:  96.41791793046399 %\n",
      "Best grid:  {'alpha': 0.1, 'fit_prior': True}\n",
      "Class:  identity_hate\n",
      "Best accuracy:  99.08003910460482 %\n",
      "Best grid:  {'alpha': 1000, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "final_hyperparameters = []\n",
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    best_score = 0\n",
    "    \n",
    "    model = MultinomialNB ()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split (X, y_train, random_state = 42, test_size = 0.25, stratify = y_train)\n",
    "    \n",
    "    X_train_sparse_matrix = count_vectorizer.fit_transform(X_train)\n",
    "    X_validation_sparse_matrix = count_vectorizer.transform(X_val)\n",
    "    \n",
    "    for g in ParameterGrid(hyperparameters):\n",
    "\n",
    "        model.set_params(**g)\n",
    "\n",
    "        model.fit(X_train_sparse_matrix, y_train)\n",
    "        predictions = model.predict (X_train_sparse_matrix)\n",
    "        train_acc = compute_accuracy (predictions, y_train)\n",
    "\n",
    "        predictions = model.predict (X_validation_sparse_matrix)\n",
    "        val_acc = compute_accuracy (predictions, y_val)\n",
    "\n",
    "        if val_acc > best_score:\n",
    "            best_score = val_acc\n",
    "            best_grid = g\n",
    "    \n",
    "    print(\"Best accuracy: \", best_score, \"%\")\n",
    "    print(\"Best grid: \", best_grid)\n",
    "    temp = mn_hyper_parameter (class_, best_grid['alpha'], best_grid['fit_prior'])\n",
    "    final_hyperparameters.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70786f3",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2fa9712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "159297f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a6c216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "94.79855362189872\n",
      "Class:  severe_toxic\n",
      "99.00357834443602\n",
      "Class:  obscene\n",
      "97.06212281680256\n",
      "Class:  threat\n",
      "99.6390321549655\n",
      "Class:  insult\n",
      "96.51315088581258\n",
      "Class:  identity_hate\n",
      "99.0580995293631\n"
     ]
    }
   ],
   "source": [
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    \n",
    "    y_train = train[class_]\n",
    "    temp = final_hyperparameters [counter]\n",
    "    model = MultinomialNB (alpha = temp.alpha, fit_prior = temp.fit_prior)\n",
    "\n",
    "    model.fit(count_train, y_train)\n",
    "    predictions = model.predict(count_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    \n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb36bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(count_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "    \n",
    "sample_submission.to_csv(f'results/submission_count_mn_tuned.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b269a8fd",
   "metadata": {},
   "source": [
    "### Logistic Regression using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb036227",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abe48db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a15268",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4b85759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4d6e7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.10580869957573\n",
      "Class:  severe_toxic\n",
      "99.09695370712723\n",
      "Class:  obscene\n",
      "97.99274304228211\n",
      "Class:  threat\n",
      "99.72676739507806\n",
      "Class:  insult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.32908861885932\n",
      "Class:  identity_hate\n",
      "99.25863722104894\n"
     ]
    }
   ],
   "source": [
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    model = LogisticRegression ()\n",
    "\n",
    "    model.fit(tf_idf_train, y_train)\n",
    "    predictions = model.predict(tf_idf_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55109e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(tf_idf_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "    \n",
    "sample_submission.to_csv(f'results/submission_tf_idf_log_reg.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba88dd",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "934900a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f6a5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [{\n",
    "    'C' : [1, 12, 15],\n",
    "    'max_iter' :[600, 1800, 3000, 4200]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce1d8f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "Best accuracy:  95.79625498207706 %\n",
      "Best grid:  {'C': 12, 'max_iter': 600}\n",
      "Class:  severe_toxic\n",
      "Best accuracy:  99.05747875567143 %\n",
      "Best grid:  {'C': 1, 'max_iter': 600}\n",
      "Class:  obscene\n",
      "Best accuracy:  97.91442107637931 %\n",
      "Best grid:  {'C': 12, 'max_iter': 600}\n",
      "Class:  threat\n",
      "Best accuracy:  99.72927581279923 %\n",
      "Best grid:  {'C': 15, 'max_iter': 600}\n",
      "Class:  insult\n",
      "Best accuracy:  97.01451382448049 %\n",
      "Best grid:  {'C': 12, 'max_iter': 600}\n",
      "Class:  identity_hate\n",
      "Best accuracy:  99.20537437645703 %\n",
      "Best grid:  {'C': 1, 'max_iter': 600}\n"
     ]
    }
   ],
   "source": [
    "final_hyperparameters = []\n",
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    best_score = 0\n",
    "    \n",
    "    model = LogisticRegression ()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split (X, y_train, test_size = 0.25, stratify = y_train)\n",
    "    \n",
    "    X_train_sparse_matrix = tf_idf_vectorizer.fit_transform(X_train)\n",
    "    X_validation_sparse_matrix = tf_idf_vectorizer.transform(X_val)\n",
    "    \n",
    "    for g in ParameterGrid(hyperparameters):\n",
    "\n",
    "        model.set_params(**g)\n",
    "\n",
    "        model.fit(X_train_sparse_matrix, y_train)\n",
    "        predictions = model.predict (X_train_sparse_matrix)\n",
    "        train_acc = compute_accuracy (predictions, y_train)\n",
    "\n",
    "        predictions = model.predict (X_validation_sparse_matrix)\n",
    "        val_acc = compute_accuracy (predictions, y_val)\n",
    "\n",
    "        if val_acc > best_score:\n",
    "            best_score = val_acc\n",
    "            best_grid = g\n",
    "    \n",
    "    print(\"Best accuracy: \", best_score, \"%\")\n",
    "    print(\"Best grid: \", best_grid)\n",
    "    temp = lr_hyperparameter (class_, best_grid['C'], best_grid['max_iter'])\n",
    "    final_hyperparameters.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ec8a5",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33fe5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f6a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf_vectorizer.fit_transform(X_train)\n",
    "tf_idf_test = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57fc540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "96.95809388924053\n",
      "Class:  severe_toxic\n",
      "99.09695370712723\n",
      "Class:  obscene\n",
      "98.56928890587888\n",
      "Class:  threat\n",
      "99.85273013266823\n",
      "Class:  insult\n",
      "97.90375444159653\n",
      "Class:  identity_hate\n",
      "99.25863722104894\n"
     ]
    }
   ],
   "source": [
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    temp = final_hyperparameters [counter]\n",
    "    model = LogisticRegression (C = temp.c, max_iter = temp.max_iter)\n",
    "\n",
    "    model.fit(tf_idf_train, y_train)\n",
    "    predictions = model.predict(tf_idf_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70ff62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(tf_idf_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "    \n",
    "sample_submission.to_csv(f'results/submission_tf_idf_log_reg_tuned.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f7246",
   "metadata": {},
   "source": [
    "### Logistic Regression using Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686cba4",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c6af596",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5e7a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "866dab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b4fbf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.05692763722732\n",
      "Class:  severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.11575411572278\n",
      "Class:  obscene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.01217013116418\n",
      "Class:  threat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.76750161370174\n",
      "Class:  insult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.88414561543138\n",
      "Class:  identity_hate\n",
      "99.19596919239711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    model = LogisticRegression ()\n",
    "\n",
    "    model.fit(count_train, y_train)\n",
    "    predictions = model.predict(count_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e560f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "96.05692763722732\n",
      "Class:  severe_toxic\n",
      "99.11575411572278\n",
      "Class:  obscene\n",
      "98.01217013116418\n",
      "Class:  threat\n",
      "99.76750161370174\n",
      "Class:  insult\n",
      "96.88414561543138\n",
      "Class:  identity_hate\n",
      "99.19596919239711\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    predictions = arr_model [counter].predict(count_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c6093fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(count_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "    \n",
    "sample_submission.to_csv(f'results/submission_count_log_reg.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408644e",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8033eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c943f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [{\n",
    "    'C' : [1, 12, 15],\n",
    "    'max_iter' :[600, 1800, 3000, 4200]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a05384cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  95.47790339157245 %\n",
      "Best grid:  {'C': 1, 'max_iter': 600}\n",
      "Class:  severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  99.04243852304916 %\n",
      "Best grid:  {'C': 1, 'max_iter': 600}\n",
      "Class:  obscene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  97.726418168601 %\n",
      "Best grid:  {'C': 1, 'max_iter': 600}\n",
      "Class:  threat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  99.66660817687314 %\n",
      "Best grid:  {'C': 1, 'max_iter': 600}\n",
      "Class:  insult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  96.48559897726419 %\n",
      "Best grid:  {'C': 1, 'max_iter': 1800}\n",
      "Class:  identity_hate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  99.06249216654551 %\n",
      "Best grid:  {'C': 1, 'max_iter': 600}\n"
     ]
    }
   ],
   "source": [
    "final_hyperparameters = []\n",
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    best_score = 0\n",
    "    \n",
    "    model = LogisticRegression ()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split (X, y_train, random_state = 42, test_size = 0.25, stratify = y_train)\n",
    "    \n",
    "    X_train_sparse_matrix = count_vectorizer.fit_transform(X_train)\n",
    "    X_validation_sparse_matrix = count_vectorizer.transform(X_val)\n",
    "    \n",
    "    for g in ParameterGrid(hyperparameters):\n",
    "\n",
    "        model.set_params(**g)\n",
    "\n",
    "        model.fit(X_train_sparse_matrix, y_train)\n",
    "        predictions = model.predict (X_train_sparse_matrix)\n",
    "        train_acc = compute_accuracy (predictions, y_train)\n",
    "\n",
    "        predictions = model.predict (X_validation_sparse_matrix)\n",
    "        val_acc = compute_accuracy (predictions, y_val)\n",
    "\n",
    "        if val_acc > best_score:\n",
    "            best_score = val_acc\n",
    "            best_grid = g\n",
    "    \n",
    "    print(\"Best accuracy: \", best_score, \"%\")\n",
    "    print(\"Best grid: \", best_grid)\n",
    "    temp = lr_hyperparameter (class_, best_grid['C'], best_grid['max_iter'])\n",
    "    final_hyperparameters.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87ee1a",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3787e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train ['comment_text']\n",
    "X_test = test ['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f612c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5e6b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  toxic\n",
      "96.16910340851408\n",
      "Class:  severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.20474271640836\n",
      "Class:  obscene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.2120811425635\n",
      "Class:  threat\n",
      "99.79068878430292\n",
      "Class:  insult\n",
      "97.1260442060274\n",
      "Class:  identity_hate\n",
      "99.28433111279618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "classes = train.columns [2:]\n",
    "arr_model = []\n",
    "counter = 0\n",
    "for class_ in classes:\n",
    "    print(\"Class: \", class_)\n",
    "    y_train = train[class_]\n",
    "    temp = final_hyperparameters [counter]\n",
    "    model = LogisticRegression (C = temp.c, max_iter = temp.max_iter)\n",
    "\n",
    "    model.fit(count_train, y_train)\n",
    "    predictions = model.predict(count_train)\n",
    "    print(compute_accuracy(predictions, y_train))\n",
    "    arr_model.append(model)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26ada8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission ['id'] = test ['id'] \n",
    "counter = 0\n",
    "\n",
    "for class_ in classes:\n",
    "    predictions = arr_model [counter].predict(count_test)\n",
    "    sample_submission [class_] = predictions\n",
    "    counter = counter + 1\n",
    "    \n",
    "sample_submission.to_csv(f'results/submission_count_log_reg_tuned.csv', index = False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
